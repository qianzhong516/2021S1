{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration\n",
    "\n",
    "The following demonstration will use the training set of the OHSUMED corpus. This training set was used in the Filtering Track of the 9th edition of the Text REtrieval Conference (TREC-9). We will use it for the information retrieval exercises of this workshop. Download [ohsumed.zip](ohsumed.zip) into the same folder as this notebook. The file is part of the git repository, so if you have cloned or downloaded the entire repository you will have the file in the right folder.\n",
    "\n",
    "The following code unzips the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('ohsumed.zip', 'r')\n",
    "zip_ref.extractall('.')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you read the data, we are providing the file ohsumed.py (in the zip file above) that has a simple API to the data. When you import it at the Python prompt, it will provide the following variables:\n",
    "\n",
    "\n",
    "1. `index`: a dictionary with document IDs as keys, and document text as values.\n",
    "2. `questions`: a dictionary with query IDs as keys, and query text as values.\n",
    "3. `answers`: a dictionary with query IDs as keys, and a set with the IDs of known relevant documents as values. This information is used for evaluation.\n",
    "\n",
    "Below are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading OHSUMED data\n"
     ]
    }
   ],
   "source": [
    "import ohsumed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54710"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohsumed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['87049087',\n",
       " '87049088',\n",
       " '87049089',\n",
       " '87049090',\n",
       " '87049091',\n",
       " '87049092',\n",
       " '87049093',\n",
       " '87049094',\n",
       " '87049095',\n",
       " '87049096']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(ohsumed.index.keys()))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some patients converted from ventricular fibrillation to organized rhythms by defibrillation-trained ambulance technicians (EMT-Ds) will refibrillate before hospital arrival. The authors analyzed 271 cases of ventricular fibrillation managed by EMT-Ds working without paramedic back-up. Of 111 patients initially converted to organized rhythms, 19 (17%) refibrillated, 11 (58%) of whom were reconverted to perfusing rhythms, including nine of 11 (82%) who had spontaneous pulses prior to refibrillation. Among patients initially converted to organized rhythms, hospital admission rates were lower for patients who refibrillated than for patients who did not (53% versus 76%, P = NS), although discharge rates were virtually identical (37% and 35%, respectively). Scene-to-hospital transport times were not predictively associated with either the frequency of refibrillation or patient outcome. Defibrillation-trained EMTs can effectively manage refibrillation with additional shocks and are not at a significant disadvantage when paramedic back-up is not available.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohsumed.index['87049087']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohsumed.questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OHSU1',\n",
       " 'OHSU10',\n",
       " 'OHSU11',\n",
       " 'OHSU12',\n",
       " 'OHSU13',\n",
       " 'OHSU14',\n",
       " 'OHSU15',\n",
       " 'OHSU16',\n",
       " 'OHSU17',\n",
       " 'OHSU18']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(ohsumed.questions.keys()))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'60 year old menopausal woman without hormone replacement therapy Are there adverse effects on lipids when progesterone is given with estrogen replacement therapy'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohsumed.questions['OHSU1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ohsumed.answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'87097544', '87157536', '87157537', '87202778', '87316316', '87316326'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohsumed.answers['OHSU1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index\n",
    "\n",
    "We are going to build an inverted index of the non-stop words with frequency higher than 5.\n",
    "\n",
    "The following code reads the files and creates a counter of all words in the corpus (including stop words). We will use NLTK's word tokeniser (read the beginning of [chapter 3 of NLTK's book](http://www.nltk.org/book/ch03.html#processing-raw-text)) to convert each document into a list of tokens. **Note that this code may take some time to run**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zhong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zhong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, collections\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt') #for tokenization\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "wordcounter = collections.Counter([w.lower() for k in ohsumed.index\n",
    "                                             for s in nltk.sent_tokenize(ohsumed.index[k])\n",
    "                                             for w in nltk.word_tokenize(s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounter.most_common(10)\n",
    "# print(wordcounter['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the inverted index of all non-stop words with frequency higher than 5. **Note that this code  may take some time to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted = dict()\n",
    "for d in ohsumed.index:\n",
    "    for w in nltk.word_tokenize(ohsumed.index[d]):\n",
    "        w = w.lower()\n",
    "        if w in stop or wordcounter[w] <= 5:\n",
    "            continue\n",
    "        if w in inverted:\n",
    "            inverted[w].add(d)\n",
    "        else:\n",
    "            inverted[w] = set([d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(inverted.keys()))[3000:3010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted['acceptability']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code saves the inverted index into a pickle file. This way we do not need to compute the inverted index again. Read [Python's documentation on pickle files](https://docs.python.org/3/library/pickle.html) for more detail. Note that the file we created is opened for writing in binary mode, following the advice of this [stackoverflow post about saving pickle files](http://stackoverflow.com/questions/13906623/using-pickle-dump-typeerror-must-be-str-not-bytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# 'wb' stands for write and binary mode\n",
    "with open('inverted.pickle', 'wb') as f:\n",
    "    pickle.dump(inverted,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code reads the pickle file and returns the list of documents that maches this Boolean query:\n",
    "\n",
    "1. (menopausal OR pregnant) AND woman AND NOT healthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('inverted.pickle', 'rb') as f:\n",
    "    inverted = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(inverted['menopausal'] | inverted['pregnant']) & inverted['woman'] - inverted['healthy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it took very little time to run the query. In general, creating the index may take some time but it is needed only once if the files do not change. Queries on the index are very fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "\n",
    "## 1. Vector Retrieval\n",
    "\n",
    "### Exercise 1.1: Boolean Information Retrieval\n",
    "\n",
    "Create an inverted index of the **NLTK Gutenberg corpus** and save it into a file \"gutenbergindex.pickle\". To create this index there is no need to look for stop words or word frequencies, since the corpus is not that large. Simply use all the words. Use this index to find the documents that match the following Boolean queries:\n",
    "\n",
    "1. Brutus OR Caesar\n",
    "2. Brutus AND NOT Caesar\n",
    "3. (Brutus AND Caesar) OR Calpurnia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import nltk\n",
    "nltk.download(\"gutenberg\")\n",
    "\n",
    "# Write your code here\n",
    "from nltk.corpus import gutenberg\n",
    "from collections import Counter\n",
    "\n",
    "inverted = dict()\n",
    "for d in gutenberg.fileids():\n",
    "    words = [w for s in nltk.sent_tokenize(gutenberg.raw(d))\n",
    "               for w in nltk.word_tokenize(s)]\n",
    "    wordcounter = Counter(words)\n",
    "    for w in wordcounter.keys():\n",
    "        if w in inverted:\n",
    "            inverted[w].add(d)\n",
    "        else:\n",
    "            inverted[w] = set([d]) #{d}\n",
    "\n",
    "with open('gutenbergindex.pickle', 'wb') as f:\n",
    "    pickle.dump(inverted, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gutenbergindex.pickle','rb') as f:\n",
    "    gutenbergindex = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for searching for Brutus OR Caesar\n",
    "gutenbergindex['Brutus'] | gutenbergindex['Caesar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for searching for Brutus AND NOT Caesar\n",
    "gutenbergindex['Brutus'] - gutenbergindex['Caesar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code for searching for (Brutus AND Caesar) OR Calpurnia\n",
    "(gutenbergindex['Brutus'] & gutenbergindex['Caesar']) | gutenbergindex['Calpurnia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: tf.idf\n",
    "\n",
    "Using scikit-learn, compute the tf.idf of all words in the OHSUMED corpus. Use the English list of stop words, and leave all other settings to their default values. In particular, do not stem the words. Pickle the resulting tf.idf vectoriser into a file tfidf.pickle. **Note that in this exercise you should use the sklearn functions, not nltk. In particular, do not use NLTK's list of stop words or its tokeniser.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code to compute the tf.idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ohsumed\n",
    "\n",
    "text = [ohsumed.index[d] for d in ohsumed.index]\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code to save the results in a pickle file\n",
    "import pickle\n",
    "with open('ohsumed-tfidf', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Sort by tf.idf\n",
    "\n",
    "Write a program that returns the words of a document with highest tf.idf score. The resulting list of words should be sorted by frequency in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_tfidf(tfidf, docID, numwords=10):\n",
    "    \"\"\"Print the words with highest tf.idf, in descending order\n",
    "    >>> best_tfidf(tfidf, '87049087', numwords=3)\n",
    "    ['rhythms', 'refibrillation', 'organized']\n",
    "    \"\"\"\n",
    "    # Write your code here\n",
    "    import ohsumed\n",
    "    text = [ohsumed.index[docID]]\n",
    "    tfidf_values = tfidf.transform(text)\n",
    "    values = tfidf_values.toarray()[0] # extract [0] because it's a 2d array\n",
    "    words = tfidf.get_feature_names()\n",
    "    match = list(zip(words, values))\n",
    "    best_tfidf = sorted(match, key=lambda item:item[1], reverse=True)[:numwords]\n",
    "    print(best_tfidf)\n",
    "    \n",
    "    # Or,\n",
    "    # vector = tfidf_values.toarray()\n",
    "    # best_tfidf = sorted(words, key=lambda word:vector[0, words.index(word)], reverse=True)[:numwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tfidf(tfidf,'87049087')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tfidf(tfidf,'87049087')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise: tf.idf cosine similarity\n",
    "\n",
    "Use the OHSUMED collection for the following exercise. Write a function that takes as a parameter a string and an optional parameter $n$ the number of results, and returns the IDs of the $n$ documents that are most relevant according to tf.idf and cosine similarity. The results are sorted in descending order of the cosine similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following funcion implements cosine similarity by using the formulas we have seen in the lectures.\n",
    "# Feel free to use sklearn's implementation of cosine similarity instead.\n",
    "\n",
    "def best_documents(querystring,n=10):\n",
    "    \"\"\"Return the indices of the best n documents using cosine similarity\n",
    "    >>> best_documents(ohsumed.questions['OHSU1'], n=3)\n",
    "    ['87285549', '87162574', '87068356']\"\"\"\n",
    "    # Write your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_documents(ohsumed.questions['OHSU1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
